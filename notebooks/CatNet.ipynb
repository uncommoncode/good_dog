{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Goal\n",
    "\n",
    "Create a network that detects cats."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load Data\n",
    "\n",
    "Data comes from [Dogs vs Cats](https://www.kaggle.com/c/dogs-vs-cats/data) dataset.\n",
    "\n",
    "![Cuter than a cat](https://kaggle2.blob.core.windows.net/competitions/kaggle/3362/media/please_dont_cheat.jpg)\n",
    "\n",
    "The way Keras looks for data is in a `data/images/<class-name>/*.jpg` path. The preprocessor in `bin/dogs_vs_cats_preprocessor.py` can create this format from the `train` file provided by the competition.\n",
    "\n",
    "However the notebook here uses cached features if available, and it's ideal to use these as they significantly speed up training."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Define Network\n",
    "\n",
    "To be sample efficient and generalize well, start with a [VGG16 Network](http://www.robots.ox.ac.uk/~vgg/research/very_deep/) trained on the large [ImageNet](http://www.image-net.org/) dataset. It has cats and dogs as part of its training distribution so it likely already has a good grasp of the features essential in detecting a cat."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create image generators that yield data for creating the feature cache.\n",
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "from good_dog.models import vgg16_image_preprocessor\n",
    "\n",
    "train_generator = ImageDataGenerator(\n",
    "    preprocessing_function=vgg16_image_preprocessor\n",
    ")\n",
    "\n",
    "test_generator = ImageDataGenerator(\n",
    "    preprocessing_function=vgg16_image_preprocessor\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Define some settings for our model\n",
    "from good_dog.models import Params\n",
    "\n",
    "params = Params(batch_size=16, image_size=(150, 150))\n",
    "classes = [\"cats\", \"dogs\"]\n",
    "n_classes = len(classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Create the bottom layers for VGG16, excluding any top dense layers.\n",
    "from keras.applications import VGG16\n",
    "\n",
    "vgg16_bottom = VGG16(include_top=False, weights='imagenet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 20000 images belonging to 2 classes.\n"
     ]
    }
   ],
   "source": [
    "# Create or load our feature cache.\n",
    "# Creating the feature cache can take a long time, so it's best to download when possible.\n",
    "from good_dog.models import FeatureCache\n",
    "from good_dog.data import require_directory\n",
    "\n",
    "require_directory(\"cache/\")\n",
    "train_feature_cache = FeatureCache(params=params, \n",
    "                                   model=vgg16_bottom, \n",
    "                                   generator=train_generator, \n",
    "                                   directory=\"../data/images/train\", \n",
    "                                   count=5000, \n",
    "                                   cache_path=\"cache/dog_v_cat_vgg16_train_features.pkl\")\n",
    "train_features = train_feature_cache.features\n",
    "train_labels = train_feature_cache.labels\n",
    "\n",
    "test_feature_cache = FeatureCache(params=params, \n",
    "                                   model=vgg16_bottom, \n",
    "                                   generator=test_generator, \n",
    "                                   directory=\"../data/images/validate\", \n",
    "                                   count=800, \n",
    "                                   cache_path=\"cache/dog_v_cat_vgg16_test_features.pkl\")\n",
    "test_features = test_feature_cache.features\n",
    "test_labels = test_feature_cache.labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the top model where we use the pretrained model from vgg16.\n",
    "# This feeds into a GAP layer with some dropout to prevent overfitting.\n",
    "from keras.layers import GlobalAveragePooling2D\n",
    "from keras.layers import Dense\n",
    "from keras.layers import Flatten\n",
    "from keras.layers import Dropout\n",
    "from keras.layers import BatchNormalization\n",
    "from keras.models import Sequential\n",
    "\n",
    "top_model = Sequential()\n",
    "top_model.add(GlobalAveragePooling2D(input_shape=train_features.shape[1:]))\n",
    "top_model.add(Dropout(0.5))\n",
    "top_model.add(Dense(n_classes, activation='softmax'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import keras.optimizers as optimizers\n",
    "import keras.losses as losses\n",
    "\n",
    "# Compile our model and set optimization strategy.\n",
    "optimizer = optimizers.RMSprop()\n",
    "top_model.compile(loss=losses.categorical_crossentropy, \n",
    "                  optimizer=optimizer, \n",
    "                  metrics=['accuracy'])\n",
    "# Print out our model below.\n",
    "top_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Train our model.\n",
    "top_model.fit(train_features,\n",
    "              train_labels,\n",
    "              epochs=30,\n",
    "              validation_data=(test_features, test_labels));"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Validate Model\n",
    "\n",
    "Check our performance on cats vs dogs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Combine the bottom and top so we can predict all the way through.\n",
    "from good_dog.models import MultiModel\n",
    "\n",
    "model = MultiModel([vgg16_bottom, top_model])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Show output prediction for an exemplar image\n",
    "from keras.preprocessing.image import load_img\n",
    "\n",
    "image_path = \"../data/external/cat.jpg\"\n",
    "img = load_img(image_path, target_size=params.image_size)\n",
    "plt.imshow(img)\n",
    "vgg_img = test_generator.flow(np.expand_dims(img, axis=0)).next()\n",
    "print \"Score:\", zip(classes, model.predict(vgg_img).flat)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Visualize Model\n",
    "Show neurons that activate highly in GAP layer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Show class activation map for our image.\n",
    "from good_dog.models import ClassActivationMap\n",
    "\n",
    "image_path = \"../data/external/dog.jpg\"\n",
    "img = load_img(image_path, target_size=params.image_size)\n",
    "plt.imshow(img)\n",
    "vgg_img = test_generator.flow(np.expand_dims(img, axis=0)).next()\n",
    "\n",
    "gap_layer = top_model.layers[0]\n",
    "dense_layer = top_model.layers[-1]\n",
    "cam = ClassActivationMap(top_model.inputs, gap_layer, dense_layer)\n",
    "vgg16_feature = model.models[0].predict(vgg_img)\n",
    "\n",
    "for class_index, name in enumerate(classes):\n",
    "    softmax_activation_map = cam.softmax_map_for_class(vgg16_feature, class_index)\n",
    "    grayscale = np.array(img).mean(axis=2)\n",
    "    plt.title(\"%s softmax activation map\" % name)\n",
    "    plt.imshow(grayscale, cmap='gray', vmin=0, vmax=255, alpha=0.5, origin='upper')\n",
    "    extent = (0, params.image_size[0], params.image_size[0], 0)\n",
    "    plt.imshow(softmax_activation_map, cmap='RdYlBu', vmax=1, vmin=0, extent=extent, alpha=0.5, origin='upper')\n",
    "    plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
